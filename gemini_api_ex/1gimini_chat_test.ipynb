{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4724a50",
   "metadata": {},
   "source": [
    "# Openai api 키 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842e53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api 발급 받아서 .env에 저장\n",
    "# Key 로딩해서 출력하기\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경 변수를 로드합니다.\n",
    "# 기본적으로 현재 스크립트가 실행되는 디렉토리 또는 그 상위 디렉토리에서 .env 파일을 찾습니다.\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "\n",
    "# 환경 변수 사용 예시\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "# print(GEMINI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293af522",
   "metadata": {},
   "source": [
    "# 기본 chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947d6ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='AI learns patterns from data to make predictions or decisions.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, url_context_metadata=None, avg_logprobs=-0.052975853284200035, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)] create_time=None response_id=None model_version='gemini-2.0-flash' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=12, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=12)], prompt_token_count=8, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=8)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=20, traffic_type=None) automatic_function_calling_history=[] parsed=None\n",
      "AI learns patterns from data to make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afd426",
   "metadata": {},
   "source": [
    "# 이미지 편집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44eb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "image = PIL.Image.open('./푸들푸들.png')\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "text_input = ('Change the color of all four poodles to white.',)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-preview-image-generation\",\n",
    "    contents=[text_input, image],\n",
    "    config=types.GenerateContentConfig(\n",
    "      response_modalities=['TEXT', 'IMAGE']\n",
    "    )\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  if part.text is not None:\n",
    "    print(part.text)\n",
    "  elif part.inline_data is not None:\n",
    "    image = Image.open(BytesIO(part.inline_data.data))\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31793a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will add a fluffy white llama standing calmly next to you in the grassy field, looking in the same direction you are.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "image = PIL.Image.open('lady.jpg')\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "text_input = ('Hi, This is a picture of me.'\n",
    "            'Can you add a llama next to me?',)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-preview-image-generation\",\n",
    "    contents=[text_input, image],\n",
    "    config=types.GenerateContentConfig(\n",
    "      response_modalities=['TEXT', 'IMAGE']\n",
    "    )\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  if part.text is not None:\n",
    "    print(part.text)\n",
    "  elif part.inline_data is not None:\n",
    "    image = Image.open(BytesIO(part.inline_data.data))\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db28893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예시 이미지가 'saved_original_poodle.png' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 임시로 원본 이미지를 '처리된 이미지'라고 가정하고 저장하는 예시\n",
    "try:\n",
    "    # 원본 이미지를 다시 로드 (실제로는 API 응답에서 생성된 이미지를 사용)\n",
    "    # 이 부분은 API 응답을 시뮬레이션하기 위함입니다.\n",
    "    # 실제 API 사용 시에는 response 객체에서 이미지를 가져옵니다.\n",
    "    if 'image' in locals() and image is not None: # 원본 이미지가 성공적으로 로드되었는지 확인\n",
    "        temp_processed_image = image.copy() # 원본을 복사해서 사용\n",
    "\n",
    "        # 이미지 보여주기 (선택 사항)\n",
    "        # temp_processed_image.show()\n",
    "\n",
    "        # 이미지 저장하기\n",
    "        save_path_simulation = \"saved_original_poodle.png\"\n",
    "        temp_processed_image.save(save_path_simulation)\n",
    "        print(f\"예시 이미지가 '{save_path_simulation}' 파일로 저장되었습니다.\")\n",
    "    else:\n",
    "        print(\"원본 이미지가 로드되지 않아 저장 예시를 실행할 수 없습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"이미지 처리 또는 저장 중 오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5402b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will add a fluffy white llama standing calmly next to you in the grassy field, ensuring it complements the peaceful atmosphere of the original image.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image = PIL.Image.open('./lady.jpg')\n",
    "\n",
    "client = genai.Client(api_key = GEMINI_API_KEY)\n",
    "\n",
    "text_input = ('Hi, This is a picture of me.'\n",
    "            'Can you add a llama next to me?',)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-preview-image-generation\",\n",
    "    contents=[text_input, image],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['TEXT', 'IMAGE']\n",
    "    )\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text is not None:\n",
    "        print(part.text)\n",
    "    elif part.inline_data is not None:\n",
    "        image = Image.open(BytesIO(part.inline_data.data))\n",
    "        image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f9605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I will add a fluffy white llama standing calmly next to you in the grassy field.\n",
      "\n",
      "\n",
      "Image saved: ./generated_images/generated_image_20250528_151805_1.jpg\n",
      "All outputs saved in: ./generated_images\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 이미지를 base64로 변환하는 함수\n",
    "def image_to_base64(image):\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='JPEG')\n",
    "    img_data = buffer.getvalue()\n",
    "    return base64.b64encode(img_data).decode()\n",
    "\n",
    "# 클라이언트 설정\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# 이미지 로드 및 변환\n",
    "image = PIL.Image.open('./lady.jpg')\n",
    "image_b64 = image_to_base64(image)\n",
    "\n",
    "text_input = ('Hi, This is a picture of me. '\n",
    "              'Can you add a llama next to me?')\n",
    "\n",
    "# API 호출\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-preview-image-generation\",\n",
    "    contents=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"parts\": [\n",
    "                {\"text\": text_input},\n",
    "                {\"inline_data\": {\n",
    "                    \"mime_type\": \"image/jpeg\",\n",
    "                    \"data\": image_b64\n",
    "                }}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['TEXT', 'IMAGE']\n",
    "    )\n",
    ")\n",
    "\n",
    "# 결과 처리 및 저장\n",
    "output_dir = './generated_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for i, part in enumerate(response.candidates[0].content.parts):\n",
    "    if part.text is not None:\n",
    "        print(f\"Generated text: {part.text}\")\n",
    "        \n",
    "        # 텍스트도 저장하고 싶다면\n",
    "        with open(f'{output_dir}/response_text_{timestamp}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(part.text)\n",
    "            \n",
    "    elif part.inline_data is not None:\n",
    "        # 이미지 데이터를 PIL Image로 변환\n",
    "        image_data = part.inline_data.data\n",
    "        \n",
    "        # base64 문자열인 경우 디코딩\n",
    "        if isinstance(image_data, str):\n",
    "            image_bytes = base64.b64decode(image_data)\n",
    "        else:\n",
    "            image_bytes = image_data\n",
    "            \n",
    "        # PIL Image로 변환\n",
    "        generated_image = Image.open(BytesIO(image_bytes))\n",
    "        \n",
    "        # 이미지 저장\n",
    "        output_filename = f'{output_dir}/generated_image_{timestamp}_{i}.jpg'\n",
    "        generated_image.save(output_filename, 'JPEG', quality=95)\n",
    "        \n",
    "        print(f\"Image saved: {output_filename}\")\n",
    "        \n",
    "        # 이미지 표시 (선택사항)\n",
    "        generated_image.show()\n",
    "\n",
    "print(f\"All outputs saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971a239",
   "metadata": {},
   "source": [
    "- 유투브"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2226f2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Sure, here is the 3-sentence summary you requested.\\n\\nThe video is a demo of multimodal live streaming within Gemini 2.0. The AI agent is shown a document with highlighted text which it is asked to read and then define a particular word within that text. The AI is then tasked with telling a story, but interrupted partway through and then asked to summarize the demo so far before ultimately reading the end card to the audience.')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, url_context_metadata=None, avg_logprobs=-1.184623040093316, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=90, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=90)], prompt_token_count=42474, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.VIDEO: 'VIDEO'>, token_count=38740), ModalityTokenCount(modality=<MediaModality.AUDIO: 'AUDIO'>, token_count=3725), ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=9)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=42564, traffic_type=None), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='models/gemini-2.0-flash',\n",
    "    contents=types.Content(\n",
    "        parts=[\n",
    "            types.Part(\n",
    "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=9hE5-98ZeCg')\n",
    "            ),\n",
    "            types.Part(text='Please summarize the video in 3 sentences.')\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d8f324e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is the 3-sentence summary you requested.\n",
      "\n",
      "The video is a demo of multimodal live streaming within Gemini 2.0. The AI agent is shown a document with highlighted text which it is asked to read and then define a particular word within that text. The AI is then tasked with telling a story, but interrupted partway through and then asked to summarize the demo so far before ultimately reading the end card to the audience.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06765958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
